# 多模态PDF文档检索
完整的工作流程要经历五个阶段：

  1.**PDF结构解析**

  2.**多模态信息提取**

  3.**MD文本转化**

  4.**文档切分**

  5.**混合检索**

实际工程中需要针对不同的文档类型进行单独优化

## 1.PDF结构解析
即识别出pdf中各元素的类型：包括各级**标题**、**正文文本**、**图片**、**表格**等元素

### 技术方案
unstructured库 + OCR模型，通过unstructured库将pdf中的每一页视为一张图像并借助OCR模型进行各类元素的识别

**unstructured**

github项目主页：

<https://github.com/Unstructured-IO/unstructured>

*提供开源组件，用于摄取和预处理图像和文本文件，例如 PDF、HTML、Word 文档等等。unstructured库的应用场景围绕着简化和优化语言学习模型 (LLM) 的数据处理工作流程。非结构化库的模块化函数和连接器构成了一个统一的系统，简化了数据摄取和预处理，使其能够适应不同的平台，并高效地将非结构化数据转换为结构化输出。*

**OCR模型**

光学字符识别模型，可以对图片上的文字进行精准识别

业内OCR模型：

| 模型 | 发布团队 | 参数规模 | 核心特点 | 优势 | 局限 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| dots.ocr | 小红书 | 1.7B | 统一 VLM 架构，版面检测 +字符识别一体化，多语种 | 精度高，结构保持好，复杂表格/论文解析表现突出 | 合并单元格等极端结构仍需微调；需 GPU 运行 | 论文、技术文档、票据类端到端解析 |
| olmOCR | Allen Institute | 7B | 高保真文档线性化，保持阅读顺序，支持表格/公式/手写体 | 可在普通 GPU/部分 CPU 上运行，文本还原度高 | 偏重文本线性化，缺乏图像语义理解 | 大规模 PDF 转文本、科研/学术批处理 |
| PaddleOCR | 百度 | 轻量模型 3-10M；高精度百 MB 级 | 工业级 OCR 工具链，覆盖检测/识别/版面分析，多语种 | CPU/移动端可运行，文档与社区完善，部署稳定 | 表格/跨页结构复杂时需规则或上层模型配合 | 工业票据识别、大规模生产环境 OCR 服务 |

## 2.多模态信息提取

提取图片、表格、流程图中的信息，针对不同类型的多模态内容，识别策略也不一样


| 场景 | 技术方案 
| :--- | :---
| 公式、发票数据、表格、手写体单方等 | OCR模型识别文本内容
| 产品原型图、流程图、数据可视化图表等 | 多模态大模型（VLM）进行图像语义理解 |

**VLM模型选型**

| 模型          | 发布团队       | 参数规模       | 类型  | 核心特点                              | 优势                                      | 局限性                       | 适用场景                                |
| ------------- | -------------- | -------------- | ----- | ------------------------------------- | ----------------------------------------- | ---------------------------- | --------------------------------------- |
| GPT-5         | OpenAI         | 百亿+          | 在线  | 原生多模态（文本/图像/音频）           | 功能最全，推理强，生态成熟                   | 成本高，语义推理差，企业级私有规 | 高阶语义推理，企业级RAG，代理任务              |
| Gemini 2.5    | Google DeepMind| 数百亿         | 在线  | 长上下文（百千万级），文本/图像/音频/视频融合 | 与搜索/Workspace深度整合，多模态能力强   | 部署受限，长上下文检查、复杂性高   | 大文本语义检索，复杂企业场景                  |
| Claude 4.1    | Anthropic      | 百亿+          | 在线  | 多步推理与工作流任务驱动                 | 成本与速度平衡，图像生成高质量            | 工程科研多任务，企业/私有环境    | 工程科研，多任务场景                          |
| InternVL 3.5  | 上海人工智能实验室 | 8B-40B         | 开源  | Cascade RL增强推理，图模型/跨模态解   | 推理理想，科研/社区资源广，图形/跨模态   | 大模型高成本，GPU运算需求大   | 科研/大模型试验，图像生成、跨模态场景            |
| Qwen3-VL      | 阿里巴巴达摩院 | 3B/7B/72B      | 开源  | 文档解析，目标定向，长视频理解          | 尺寸覆盖广，性格灵活                      | 大尺度推理，性能需求高        | 企业文档，语言跨模态应用                        |
| SmolVLM       | Hugging Face 社区 | 1B-2B         | 开源  | 轻量级VLM，低算力运行为特点             | 部署门槛低，适合个人教育                   | 复杂任务弱，效果差            | 复合任务教学，轻量个人项目                        |
| Gemma 3       | Google DeepMind | 4B/12B/27B     | 开源  | 轻量到中型参数，图像/问答/图表解析       | 成本低，生态完整                          | 性能弱于大模型，超大模型         | 成本敏感/感知型企业/科教试验场                    |



## 3.MD文本转化

主要是将pdf转化为MD文档

将表格、公式等多模态信息转化为特定的MD语法，并将其他图片内容单独进行存储，以路径的方式在文档中进行引用，保证文档既能保留原始结构化信息，同时方便后续进行知识检索与展示

**pdf转Markdown解决方案**

| 工具       | 发布团队                      | 许可证  | 支持格式                 | 核心特点                                      | 优势                                   | 局限                           | 适用场景                          |
| ---------- | ----------------------------- | ------- | ------------------------ | --------------------------------------------- | -------------------------------------- | ------------------------------ | --------------------------------- |
| MinerU     | 阿里巴巴达摩院 & OpenDataLab   | AGPL-3.0 | PDF/图像                 | 集成 OCR+版面解析，高精度公式/表格支持         | 输出精细，科研 PDF 友好，CLI 简洁       | 商用许可受限，复杂排版仍需人工校正 | 科研论文/技术文档批量 Markdown 转换 |
| Docling    | IBM Research                  | MIT     | PDF/Word/PPT/HTML 等     | 企业级文档解析，支持嵌入 VLM，输出 Markdown/JSON | 多格式支持，工程化完备，可离线运行       | 默认复杂，需自定义管线           | 企业合规环境，知识库构建            |
| MarkItDown | Microsoft                     | MIT     | PDF/Office/图片/网页/音频等 | 轻量通用工具，插件可接入 Azure/LLM            | 格式覆盖广，插件灵活，部署简单           | 精排不及 MinerU，偏重可读性       | 快速预处理，通用 RAG 项目集成        |


## 4.文档切分
chunk + embedding 的环节，chunk主要是切分策略上要根据不同场景进行动态切分，核心是平衡检索精准度和上下文完整性，embedding要考虑召回率，是否在数据集上做微调

**文档切分策略**：

| 策略编号 | 策略名称 | 核心思想 | 优点 | 缺点 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1** | 固定大小分块 + Overlap | 设定一个 chunk_size（如 500 个字符）和一个 chunk_overlap（如 50 个字符）。从文本开头取 chunk_size 个字符作为第一个块，然后下一次从 `start_index + chunk_size - chunk_overlap` 的位置开始取下一个块，依此类推。 | • 实现极其简单，几乎不需要复杂的逻辑。<br>• 计算开销非常小，处理速度快。<br>• 对文本格式没有特殊要求。 | • 极易破坏语义完整性：非常可能在句子中间、单词中间、代码行中间等不恰当的地方断开，导致上下文严重割裂。<br>• 忽略文本结构：完全无视段落、标题、列表等任何文本固有结构。<br>• 固定大小对于信息密度不同、语言不同的文本效果可能差异巨大。 | • 对文本结构要求不高的简单场景。<br>• 数据量极大，需要快速进行初步处理时。<br>• 作为更复杂分块策略（如递归分块）的最后“兜底”手段。<br>• 对上下文完整性要求不高的检索任务。 |
| **2** | 基于句子的分块 | 先切分成句子，再合并句子成块。可以简单地每个句子是一个块，也可以设定一个目标块大小，将连续的句子合并，直到接近该大小。同样可以引入句子级别的重叠。 | • 更好地保持语义完整性：因为句子是表达相对完整意思的基本单位，所以很少会在句子内部断开。<br>• 比固定大小分块更符合自然语言的结构。 | • 句子长度差异大：有的句子很短，有的很长，导致 Chunk 大小不均匀，可能影响后续处理和检索稳定性。<br>• 简单的基于标点的分割可能不准确（例如，Mr. Smith 中的 .）。需要更可靠的 NLP 工具。<br>• 对于代码、列表、或者没有明确句子结构的文本效果不佳。<br>• 跨越多个句子的复杂语义关系可能仍然被切断。 | • 处理结构良好、以完整句子为主的文本，如新闻文章、报告、小说等。<br>• 当保持句子层面的语义完整性比较重要时。 |
| **3** | 递归字符分块 | 提供一个分隔符列表，按优先级从高到低尝试分割。优先尝试按 `\n\n` (段落) 分割，如果分割后的块仍然太大，再尝试按 `\n` (换行符) 分割，然后按空格分割，最后如果还太大，就按字符分割。目标是保持较大语义块的同时，确保最终块大小不超过限制。 | • 试图保持语义结构：优先使用段落、换行等更有意义的分隔符，尽可能维持文本的逻辑结构。<br>• 比固定大小更智能：避免在不必要的地方断开。<br>• 适应性强：对不同类型的文本结构有一定适应性，是固定大小和句子分割的一种折中和改进。 | • 实现相对复杂一些。<br>• 效果依赖于分隔符列表的选择和优先级顺序。<br>• 对于没有明显分隔符的密集文本，可能最终退化为按字符分割。 | • 通用性较好，适用于多种类型的文本文档。<br>• 当希望在控制块大小的同时尽可能保留文本结构时。<br>• 许多 RAG 框架的默认或推荐选项。 |
| **4** | 基于文档结构的分块 | 解析文档的结构树或特定标记，基于这些结构元素来定义 Chunks。例如，每个 `<p>` 标签内容作为一个 Chunk，或者每个 Markdown 的二级标题下的所有内容作为一个 Chunk。 | • 高度尊重原文结构：能够最好地保持作者组织信息的方式。<br>• 语义连贯性强：通常一个结构元素包含一个相对独立的语义单元。<br>• 可以方便地将结构信息（如标题、标签）作为元数据附加到 Chunk 上，对后续检索很有帮助。 | • 依赖于清晰、一致的文档结构：如果文档结构混乱或没有明确标记，则效果不佳。<br>• 不同结构元素的文本量可能差异巨大，导致 Chunk 大小极不均匀。<br>• 需要针对不同的文档格式编写不同的解析逻辑。 | • 处理具有清晰、标准化结构的文档，如网页、Markdown 文档、结构化数据等。<br>• 当需要利用文档结构信息进行检索或过滤时。 |
| **5** | 混合分块 | 分层处理。先用结构化或语义边界做粗粒度切分，得到逻辑上相关的“大块”，再对这些“大块”应用更细粒度的策略来满足大小限制，同时保留“大块”的上下文信息作为元数据。 | • 兼顾结构与大小限制：既能利用文档的宏观结构，又能确保最终块大小可控。<br>• 元数据丰富：可以方便地继承来自结构化分割的元数据。<br>• 灵活性高，可根据需求定制组合策略。 | • 实现复杂度相对较高。<br>• 需要仔细设计组合逻辑和参数。 | • 对分块质量要求较高，希望在保持上下文、利用结构和控制大小之间取得良好平衡的场景。<br>• 处理像 Markdown、富文本文档这样既有结构又有自由文本的内容。 |
| **6** | 语义分块 | 计算相邻句子或小段文字的 Embedding 向量，当语义相似度低于设定阈值时，在此“语义断裂点”进行切割。 | • 切分点更“懂”语义，总能在话题自然转变的地方下手。<br>• 能保证每个 Chunk 内部意思高度相关，切出来的块更符合人的阅读理解习惯。 | • 要算 Embedding，计算开销大。<br>• 效果依赖 Embedding 模型本身的能力。<br>• “语义距离阈值”需实验调整。<br>• 处理速度较慢。 | • 对分块质量要求很高，并且计算资源比较充裕的场景。<br>• 适合处理没有结构化标记但意思很丰富的长文（如纯文本、对话记录）。 |
| **7** | 分层分块 | 系统化地创建多个层级的 Chunks。例如，先将文档按章节分割成大块，再将每个章节按段落分割成中块，最后可能按句子分割成小块。不同层级的 Chunks 可用于不同的检索策略。 | • 提供了不同粒度的上下文信息，增加了检索的灵活性。 | • 增加了索引的复杂度和存储空间。<br>• 需要设计好多层级之间的关系和使用方式。 | • 需要处理具有清晰层级结构的复杂文档（如书籍、长篇报告），并且希望在检索时能灵活选择上下文粒度。 |
| **8** | Small-to-big检索/父文档检索器 | 将文档分割成小块进行精确向量检索，同时保留或链接到这些小块所属的更大的父块。检索时，先用查询向量匹配小块，一旦找到相关的小块，不直接将小块传递给 LLM，而是返回其对应的父块。 | • 结合了小块的检索精度和大块的上下文完整性。<br>• 既能精准定位相关信息点，又能为 LLM 提供更丰富的背景。 | • 需要维护小块和父块之间的映射关系，增加了索引和检索逻辑的复杂度。 | • 既需要高精度检索，又需要充分上下文来生成高质量答案的 RAG 应用。 |
| **9** | 命题分块 | 尝试将文本分解为更小的、原子性的事实陈述或主张。通常需要借助 LLM 或专门的 NLP 模型来识别和提取文本中的核心命题，然后对这些命题进行索引和检索。 | • 产生了非常细粒度、高度聚焦的知识单元，非常适合进行精确的事实检索和问答。 | • 严重依赖 LLM 或 NLP 模型的抽取能力，可能产生错误或不完整的命题。<br>• 计算成本高昂，处理速度慢。<br>• 可能丢失原始文本的语气、复杂关系和细微差别。<br>• 检索到的离散命题难以有效组合。 | • 知识库构建、事实性问答系统，对信息的原子性和精确性要求极高的场景。 |
| **10** | Agentic/LLM Based Chunking | 让一个智能体或直接使用 LLM 来决策如何进行分块。可以给 LLM 设计特定的 Prompt，让它根据内容理解来判断最佳的分割点，或者让 Agent 动态地选择和组合不同的分块策略。 | • 潜力巨大，理论上可以实现最智能、最符合语义和上下文的分块。 | • 实现非常复杂，需要精巧的 Prompt Engineering 或 Agent 逻辑设计。<br>• 成本高（LLM 调用开销），速度慢。<br>• 结果的可控性和稳定性可能不如确定性算法。<br>• 仍处于探索阶段，成熟度和可靠性有待验证。 | • 研究探索性质的项目，或者对分块质量有极致追求且不计成本的特定应用。 |
| **补充** | 上下文富化 | 这不是一种独立的分割方法，而是在分块之后，为每个 Chunk 添加额外的上下文信息，如相邻的句子、摘要信息或所属章节的标题作为元数据。 | • 在不显著增大 Chunk 大小的情况下，为 LLM 提供更多线索，帮助其理解 Chunk 在原文中的位置和背景。 | • 需要额外的处理步骤来提取和添加这些富化信息。 | • 可以与其他任何分块策略结合使用，作为一种优化手段，特别是在 Chunk 较小，担心上下文不足时。 |

## 5.混合检索

现在主流的检索方式是Agentic RAG，Agentic RAG（基于智能体的RAG实现）是通过引入智能体框架来改变处理问答方式的技术。Agentic RAG利用智能体来应对需要复杂规划、多步骤推理和外部工具使用的复杂问题。

一个标准的Agentic RAG系统至少需要包含**内容护栏功能**和**检索问题增强功能**，前者用于过滤铭感信息、保证回答合规性，后者用于自动改写或扩展用户问题以提升检索召回率与语义匹配度